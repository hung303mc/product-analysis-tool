import streamlit as st
import google.generativeai as genai
import csv
import time
import json
import os
from datetime import datetime
import io
import pandas as pd

# --- PAGE CONFIG ---
st.set_page_config(
    page_title="Product Analysis Tool",
    page_icon="üöÄ",
    layout="wide"
)

# --- GOOGLE API SETUP ---
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
except (AttributeError, KeyError):
    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')

if not GOOGLE_API_KEY:
    st.error("API Key not found! Please set it in Streamlit's secrets or as an environment variable and restart.")
    st.stop()

genai.configure(api_key=GOOGLE_API_KEY)

# --- PROMPT TEMPLATE ---
DEFAULT_PROMPT = """
M√†y l√† m·ªôt nh√† chi·∫øn l∆∞·ª£c t√¨m ki·∫øm c∆° h·ªôi trong th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠. M·ª•c ti√™u ch√≠nh c·ªßa m√†y l√† x√°c ƒë·ªãnh c√°c s·∫£n ph·∫©m "winner" M·ªöI cho m·ªôt doanh nghi·ªáp online linh ho·∫°t.

B·ªêI C·∫¢NH & QUY T·∫ÆC KINH DOANH:
1.  **M√¥ h√¨nh Logistics:** V·∫≠n chuy·ªÉn h√†ng kh√¥ng (Air freight) t·ª´ Trung Qu·ªëc v·ªÅ kho ·ªü M·ªπ. S·∫£n ph·∫©m kh√¥ng l∆∞u kho d√†i h·∫°n.
2.  **GI·ªöI H·∫†N C√ÇN N·∫∂NG NGHI√äM NG·∫∂T:** S·∫£n ph·∫©m > 4 lbs (~1.8 kg) s·∫Ω t·ª± ƒë·ªông b·ªã 'B·ªè qua' (Skip). L√Ω t∆∞·ªüng l√† d∆∞·ªõi 3 lbs (~1.4 kg).
3.  **L·ª£i nhu·∫≠n l√† tr√™n h·∫øt:** S·ª≠ d·ª•ng chi ph√≠ v·∫≠n chuy·ªÉn tham kh·∫£o (~$15 cho 1lb, ~$25 cho 2lbs, v.v.) ƒë·ªÉ ∆∞·ªõc t√≠nh l·ª£i nhu·∫≠n r√≤ng ti·ªÅm nƒÉng.
4.  **TH√ÄNH C√îNG BAN ƒê·∫¶U (ƒë·ªÉ tham kh·∫£o, kh√¥ng ph·∫£i gi·ªõi h·∫°n):** Doanh nghi·ªáp ƒë√£ c√≥ th√†nh c√¥ng b∆∞·ªõc ƒë·∫ßu v·ªõi ng√°ch "C√°c thi·∫øt b·ªã cho th·ªùi ti·∫øt n√≥ng" v√† "Ph·ª• ki·ªán th·ªùi trang m√πa h√®". H√£y d√πng th√¥ng tin n√†y l√†m ng·ªØ c·∫£nh ƒë·ªÉ nh·∫≠n di·ªán c√°c m√¥ th·ª©c th√†nh c√¥ng (v√≠ d·ª•: s·∫£n ph·∫©m ƒë·ªôc ƒë√°o gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ), nh∆∞ng TUY·ªÜT ƒê·ªêI KH√îNG ch·∫•m ƒëi·ªÉm th·∫•p cho c√°c s·∫£n ph·∫©m thu·ªôc ng√†nh h√†ng m·ªõi. M·ª•c ti√™u ch√≠nh l√† t√¨m ra NG√ÅCH TH√ÄNH C√îNG TI·∫æP THEO.
5.  **M·ª•c ti√™u c·ªët l√µi:** T√¨m ki·∫øm c√°c s·∫£n ph·∫©m ƒë·ªôc ƒë√°o, c√≥ ti·ªÅm nƒÉng viral, c√≥ th·ªÉ t√¨m ngu·ªìn t·ª´ 1688.

NHI·ªÜM V·ª§:
Ph√¢n t√≠ch L√î s·∫£n ph·∫©m d∆∞·ªõi ƒë√¢y v·ªõi t∆∞ duy c·ªüi m·ªü. M·ª•c ti√™u ch√≠nh c·ªßa m√†y l√† x√°c ƒë·ªãnh c√°c s·∫£n ph·∫©m "winner" ti·ªÅm nƒÉng ti·∫øp theo, b·∫•t k·ªÉ ch√∫ng thu·ªôc danh m·ª•c n√†o. Cung c·∫•p m·ªôt "overall_summary" chi·∫øn l∆∞·ª£c v√† sau ƒë√≥ ƒë√°nh gi√° T·ª™NG s·∫£n ph·∫©m.

D·ªØ li·ªáu l√¥ s·∫£n ph·∫©m (ƒë·ªãnh d·∫°ng CSV):
{product_batch_csv}

Ph√¢n t√≠ch v√† tr·∫£ l·ªùi trong m·ªôt ƒë·ªãnh d·∫°ng JSON h·ª£p l·ªá duy nh·∫•t. KH√îNG th√™m b·∫•t k·ª≥ vƒÉn b·∫£n n√†o b√™n ngo√†i ƒë·ªëi t∆∞·ª£ng JSON ch√≠nh.

C·∫•u tr√∫c ph·∫£n h·ªìi JSON ph·∫£i nh∆∞ sau:
{{
  "overall_summary": "string (T√≥m t·∫Øt chi·∫øn l∆∞·ª£c c·ªßa m√†y v·ªÅ l√¥ h√†ng, nh·∫•n m·∫°nh v√†o c√°c C∆† H·ªòI M·ªöI t·ªët nh·∫•t. Cung c·∫•p ph·∫ßn n√†y b·∫±ng ti·∫øng Vi·ªát.)",
  "product_analyses": [
    {{
      "asin": "string (ASIN c·ªßa s·∫£n ph·∫©m)",
      "classification": "string (Gi·ªØ nguy√™n c√°c thu·∫≠t ng·ªØ ti·∫øng Anh n√†y: Winner, High Potential, Potential, Skip)",
      "reason": "string (L√Ω do to√†n di·ªán c·ªßa m√†y, gi·∫£i th√≠ch t·∫°i sao ƒë√¢y c√≥ th·ªÉ l√† m·ªôt s·∫£n ph·∫©m win, ngay c·∫£ khi n√≥ ·ªü trong m·ªôt ng√°ch m·ªõi. Cung c·∫•p ph·∫ßn n√†y b·∫±ng ti·∫øng Vi·ªát.)",
      "viral_potential": "string (Gi·ªØ nguy√™n c√°c thu·∫≠t ng·ªØ ti·∫øng Anh n√†y: Low, Medium, High)",
      "uniqueness_score": "number (1-10, 1 l√† h√†ng ph·ªï th√¥ng, 10 l√† c·ª±c k·ª≥ ƒë·ªôc ƒë√°o)",
      "market_trend_score": "number (1-10, s·∫£n ph·∫©m n√†y ph√π h·ª£p v·ªõi c√°c xu h∆∞·ªõng hi·ªán t·∫°i ho·∫∑c m·ªõi n·ªïi ·ªü th·ªã tr∆∞·ªùng M·ªπ nh∆∞ th·∫ø n√†o)",
      "logistics_fit": "string (Gi·ªØ nguy√™n c√°c thu·∫≠t ng·ªØ ti·∫øng Anh n√†y: Good, Acceptable, Poor)",
      "estimated_shipping_cost": "number (∆Ø·ªõc t√≠nh c·ªßa m√†y b·∫±ng USD)",
      "profit_potential": "string (Gi·ªØ nguy√™n c√°c thu·∫≠t ng·ªØ ti·∫øng Anh n√†y: Low, Medium, High, Very High)",
      "risks": ["string (Li·ªát k√™ c√°c r·ªßi ro ch√≠nh b·∫±ng ti·∫øng Vi·ªát.)"]
    }}
  ]
}}
"""

# --- CORE FUNCTIONS (Logic x·ª≠ l√Ω ch√≠nh) ---

def analyze_product_batch_api(product_batch_list, model, prompt_template):
    if not product_batch_list:
        return None
    
    output = io.StringIO()
    fieldnames = product_batch_list[0].keys()
    writer = csv.DictWriter(output, fieldnames=fieldnames)
    writer.writeheader()
    writer.writerows(product_batch_list)
    csv_string = output.getvalue()

    prompt = prompt_template.format(product_batch_csv=csv_string)
    
    try:
        response = model.generate_content(prompt)
        if response.parts:
            cleaned_text = response.text.strip().replace('```json', '').replace('```', '')
            return json.loads(cleaned_text)
        else:
            return None
    except Exception as e:
        st.error(f"L·ªói API ho·∫∑c JSON: {e}")
        return None

# --- STREAMLIT APP UI ---

st.title("üöÄ C√¥ng C·ª• Ph√¢n T√≠ch S·∫£n Ph·∫©m")
st.info("Upload file CSV s·∫£n ph·∫©m, ch·ªânh s·ª≠a prompt n·∫øu c·∫ßn, sau ƒë√≥ nh·∫•n 'B·∫Øt ƒë·∫ßu Ph√¢n t√≠ch'.")

# <<< M·ª§C M·ªöI 1: H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG >>>
with st.expander("üìñ Xem h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng (Workflow)", expanded=False):
    st.markdown("""
    **C√¥ng c·ª• n√†y ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ l√†m vi·ªác v·ªõi file CSV ƒë∆∞·ª£c xu·∫•t ra t·ª´ tool Black Box c·ªßa Helium 10.**

    #### **Quy tr√¨nh l√†m vi·ªác:**
    1.  **üì• L·∫•y File Input:**
        * V√†o **Helium 10 Black Box**, l·ªçc s·∫£n ph·∫©m theo c√°c ti√™u ch√≠ m√†y mu·ªën.
        * Nh·∫•n n√∫t **"Export Data"** ƒë·ªÉ t·∫£i v·ªÅ file CSV.

    2.  **‚¨ÜÔ∏è T·∫£i File L√™n ƒê√¢y:**
        * K√©o v√† th·∫£ ho·∫∑c nh·∫•n v√†o n√∫t 'Browse files' ƒë·ªÉ t·∫£i file CSV v·ª´a download t·ª´ Helium 10 l√™n.

    3.  **‚öôÔ∏è (T√πy ch·ªçn) Tinh Ch·ªânh:**
        * ·ªû thanh b√™n tr√°i, m√†y c√≥ th·ªÉ ch·ªçn Model AI ho·∫∑c ƒëi·ªÅu ch·ªânh 'Batch Size' (s·ªë s·∫£n ph·∫©m x·ª≠ l√Ω m·ªói l·∫ßn).
        * N·∫øu c·∫ßn, m√†y c√≥ th·ªÉ s·ª≠a ƒë·ªïi Prompt ƒë·ªÉ thay ƒë·ªïi c√°ch AI ph√¢n t√≠ch.

    4.  **üöÄ B·∫Øt ƒê·∫ßu Ph√¢n T√≠ch:**
        * Nh·∫•n v√†o n√∫t **'B·∫Øt ƒë·∫ßu Ph√¢n t√≠ch'** v√† ch·ªù c√¥ng c·ª• x·ª≠ l√Ω.
        * C√¥ng c·ª• s·∫Ω ƒë·ªçc file c·ªßa m√†y, g·ª≠i d·ªØ li·ªáu cho AI ph√¢n t√≠ch, v√† t·∫°o ra m·ªôt file k·∫øt qu·∫£ m·ªõi.

    5.  **üìÑ Xem & T·∫£i K·∫øt Qu·∫£:**
        * File output s·∫Ω bao g·ªìm c√°c c·ªôt d·ªØ li·ªáu g·ªëc quan tr·ªçng v√† c√°c c·ªôt ph√¢n t√≠ch m·ªõi t·ª´ AI (nh∆∞ `classification`, `reason`, `viral_potential`...).
        * Nh·∫•n n√∫t **'T·∫£i v·ªÅ file k·∫øt qu·∫£'** ƒë·ªÉ l∆∞u file CSV ƒë√£ ƒë∆∞·ª£c l√†m gi√†u th√¥ng tin v·ªÅ m√°y.
    """)


# -- Sidebar ch·ª©a c√°c c√†i ƒë·∫∑t --
st.sidebar.header("C√†i ƒë·∫∑t")
model_name = st.sidebar.selectbox(
    "Ch·ªçn Model Gemini", 
    ('gemini-2.5-pro', 'gemini-1.5-pro', 'gemini-1.5-flash'),
    index=0, 
    help="2.5 Pro cho ch·∫•t l∆∞·ª£ng ph√¢n t√≠ch cao nh·∫•t."
)
batch_size = st.sidebar.slider(
    "S·ªë s·∫£n ph·∫©m m·ªói l√¥ (Batch Size)", 
    min_value=5, max_value=20, value=10,
    help="S·ªë l∆∞·ª£ng s·∫£n ph·∫©m g·ª≠i cho AI trong m·ªôt l·∫ßn. L·ªõn h∆°n gi√∫p AI so s√°nh t·ªët h∆°n."
)

# -- √î s·ª≠a prompt --
with st.expander("üìù Ch·ªânh s·ª≠a Prompt (N√¢ng cao)"):
    prompt_text = st.text_area("Prompt Template:", value=DEFAULT_PROMPT, height=400)

# -- Khu v·ª±c Upload file --
uploaded_file = st.file_uploader("Ch·ªçn file CSV c·ªßa m√†y:", type=["csv"])

if uploaded_file is not None:
    try:
        df = pd.read_csv(uploaded_file)
        st.success(f"ƒê√£ t·∫£i l√™n th√†nh c√¥ng file '{uploaded_file.name}' v·ªõi {len(df)} s·∫£n ph·∫©m.")
        
        st.dataframe(df.head())

        # <<< M·ª§C M·ªöI 2: ∆Ø·ªöC T√çNH TH·ªúI GIAN >>>
        num_products = len(df)
        # D·ª±a tr√™n ∆∞·ªõc t√≠nh th·ª±c t·∫ø: 1 l√¥ 10 s·∫£n ph·∫©m v·ªõi model Pro m·∫•t ~60 gi√¢y
        time_per_batch_seconds = 60 
        num_batches = (num_products + batch_size - 1) // batch_size
        total_seconds = num_batches * time_per_batch_seconds
        
        # Chuy·ªÉn th√†nh ph√∫t v√† gi√¢y ƒë·ªÉ d·ªÖ ƒë·ªçc
        minutes, seconds = divmod(int(total_seconds), 60)
        
        st.markdown(f"---")
        st.markdown(f"‚è≥ **Th·ªùi gian x·ª≠ l√Ω d·ª± ki·∫øn:** Kho·∫£ng **{minutes} ph√∫t {seconds} gi√¢y** cho `{num_products}` s·∫£n ph·∫©m (v·ªõi batch size l√† `{batch_size}`).")
        st.caption("Th·ªùi gian th·ª±c t·∫ø c√≥ th·ªÉ thay ƒë·ªïi t√πy v√†o t·∫£i c·ªßa API v√† ƒë·ªô ph·ª©c t·∫°p c·ªßa d·ªØ li·ªáu.")


        if st.button("B·∫Øt ƒë·∫ßu Ph√¢n t√≠ch", type="primary", use_container_width=True):
            model = genai.GenerativeModel(model_name)
            all_products = df.to_dict('records')
            
            final_results = []
            
            progress_bar = st.progress(0, text="ƒêang chu·∫©n b·ªã...")
            
            batches = [all_products[i:i + batch_size] for i in range(0, len(all_products), batch_size)]
            
            for i, batch in enumerate(batches):
                status_text = f"ƒêang x·ª≠ l√Ω l√¥ {i+1}/{len(batches)}... Vui l√≤ng ch·ªù."
                progress_bar.progress((i) / len(batches), text=status_text)
                
                if not batch:
                    continue

                analysis_result = analyze_product_batch_api(batch, model, prompt_text)
                time.sleep(2)

                if analysis_result and 'product_analyses' in analysis_result:
                    batch_map = {row.get('ASIN'): row for row in batch if row.get('ASIN')}
                    for analysis in analysis_result['product_analyses']:
                        asin = analysis.get('asin')
                        if asin in batch_map:
                            original_row = batch_map[asin]
                            original_row.update(analysis)
                            if isinstance(original_row.get('risks'), list):
                                original_row['risks'] = ', '.join(original_row['risks'])
                            final_results.append(original_row)
                else:
                    st.warning(f"L√¥ {i+1} x·ª≠ l√Ω th·∫•t b·∫°i ho·∫∑c kh√¥ng c√≥ k·∫øt qu·∫£.")
                    for row in batch:
                        row['classification'] = 'API_ERROR'
                        final_results.append(row)

            progress_bar.progress(1.0, text="Ph√¢n t√≠ch ho√†n t·∫•t!")
            
            result_df = pd.DataFrame(final_results)
            
            st.subheader("K·∫øt qu·∫£ Ph√¢n t√≠ch")
            st.dataframe(result_df)
            
            csv_output = result_df.to_csv(index=False, encoding='utf-8-sig')
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_filename = f"{timestamp}_analyzed_{uploaded_file.name}"

            st.download_button(
                label="üì• T·∫£i v·ªÅ file k·∫øt qu·∫£",
                data=csv_output,
                file_name=output_filename,
                mime='text/csv',
                use_container_width=True
            )
            
    except Exception as e:
        st.error(f"ƒê√£ c√≥ l·ªói x·∫£y ra khi ƒë·ªçc ho·∫∑c x·ª≠ l√Ω file: {e}")