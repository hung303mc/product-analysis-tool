import streamlit as st
import google.generativeai as genai
import gspread
from gspread_dataframe import set_with_dataframe
from google.oauth2.service_account import Credentials
import csv
import time
import json
import os
from datetime import datetime
import io
import pandas as pd
import numpy as np
import logging # <<< DEBUG LOG >>> Th√™m th∆∞ vi·ªán logging

# <<< DEBUG LOG >>> C·∫•u h√¨nh logging ƒë·ªÉ in ra console
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


# --- PAGE CONFIG ---
st.set_page_config(page_title="Product Analysis Tool", page_icon="üöÄ", layout="wide")

# --- AUTHENTICATION & SETUP ---
# Thi·∫øt l·∫≠p cho c·∫£ Gemini v√† Google Sheets
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=GOOGLE_API_KEY)
    
    GCP_CREDS = st.secrets["gcp_service_account"]
    SCOPES = ['https://www.googleapis.com/auth/spreadsheets']
    creds = Credentials.from_service_account_info(GCP_CREDS, scopes=SCOPES)
    gc = gspread.authorize(creds)

except Exception as e:
    st.error(f"L·ªói c√†i ƒë·∫∑t ho·∫∑c x√°c th·ª±c: {e}")
    st.info("H√£y ki·ªÉm tra l·∫°i file .streamlit/secrets.toml v√† ch·∫Øc ch·∫Øn m√†y ƒë√£ c·∫•u h√¨nh ƒë·ªß c·∫£ GOOGLE_API_KEY v√† gcp_service_account.")
    st.stop()

# --- PROMPT TEMPLATE (Gi·ªØ nguy√™n nh∆∞ code g·ªëc c·ªßa m√†y) ---
DEFAULT_PROMPT = """
M√†y l√† m·ªôt nh√† chi·∫øn l∆∞·ª£c t√¨m ki·∫øm c∆° h·ªôi trong th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠. M·ª•c ti√™u ch√≠nh c·ªßa m√†y l√† x√°c ƒë·ªãnh c√°c s·∫£n ph·∫©m "winner" M·ªöI cho m·ªôt doanh nghi·ªáp online linh ho·∫°t, ph√π h·ª£p v·ªõi chi·∫øn l∆∞·ª£c tƒÉng tr∆∞·ªüng 2 giai ƒëo·∫°n.

B·ªêI C·∫¢NH & QUY T·∫ÆC KINH DOANH:
1.  **M√¥ h√¨nh Logistics:** - Giai ƒëo·∫°n 1 (Test): B√°n h√†ng theo h√¨nh th·ª©c Dropshipping. S·∫£n ph·∫©m ƒë∆∞·ª£c ship l·∫ª tr·ª±c ti·∫øp t·ª´ nh√† cung c·∫•p (c·ª• th·ªÉ l√† t·ª´ Trung Qu·ªëc) ƒë·∫øn kh√°ch h√†ng ·ªü M·ªπ ƒë·ªÉ ki·ªÉm ch·ª©ng nhu c·∫ßu th·ªã tr∆∞·ªùng m√† kh√¥ng c·∫ßn v·ªën nh·∫≠p h√†ng.
- Giai ƒëo·∫°n 2 (Scale): Khi m·ªôt s·∫£n ph·∫©m cho th·∫•y t√≠n hi·ªáu t·ªët (doanh s·ªë, ph·∫£n h·ªìi t√≠ch c·ª±c), s·∫Ω ti·∫øn h√†nh nh·∫≠p m·ªôt l√¥ h√†ng nh·ªè (100-500 s·∫£n ph·∫©m) v·ªÅ kho t·∫°i M·ªπ ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô giao h√†ng (FBM ho·∫∑c FBA) v√† tƒÉng bi√™n l·ª£i nhu·∫≠n.
2.  **GI·ªöI H·∫†N C√ÇN N·∫∂NG NGHI√äM NG·∫∂T:** S·∫£n ph·∫©m > 4 lbs (~1.8 kg) s·∫Ω t·ª± ƒë·ªông b·ªã 'B·ªè qua' (Skip). L√Ω t∆∞·ªüng l√† d∆∞·ªõi 3 lbs (~1.4 kg).
3.  **L·ª£i nhu·∫≠n l√† tr√™n h·∫øt:** S·ª≠ d·ª•ng chi ph√≠ v·∫≠n chuy·ªÉn tham kh·∫£o (~$15 cho 1lb, ~$25 cho 2lbs, v.v.) ƒë·ªÉ ∆∞·ªõc t√≠nh l·ª£i nhu·∫≠n r√≤ng ti·ªÅm nƒÉng.
4.  **TH√ÄNH C√îNG BAN ƒê·∫¶U (ƒë·ªÉ tham kh·∫£o, kh√¥ng ph·∫£i gi·ªõi h·∫°n):** Doanh nghi·ªáp ƒë√£ c√≥ th√†nh c√¥ng b∆∞·ªõc ƒë·∫ßu v·ªõi ng√°ch "C√°c thi·∫øt b·ªã cho th·ªùi ti·∫øt n√≥ng" v√† "Ph·ª• ki·ªán th·ªùi trang m√πa h√®". H√£y d√πng th√¥ng tin n√†y l√†m ng·ªØ c·∫£nh ƒë·ªÉ nh·∫≠n di·ªán c√°c m√¥ th·ª©c th√†nh c√¥ng (v√≠ d·ª•: s·∫£n ph·∫©m ƒë·ªôc ƒë√°o gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ), nh∆∞ng TUY·ªÜT ƒê·ªêI KH√îNG ch·∫•m ƒëi·ªÉm th·∫•p cho c√°c s·∫£n ph·∫©m thu·ªôc ng√†nh h√†ng m·ªõi. M·ª•c ti√™u ch√≠nh l√† t√¨m ra NG√ÅCH TH√ÄNH C√îNG TI·∫æP THEO.
5.  **M·ª•c ti√™u c·ªët l√µi:** T√¨m ki·∫øm c√°c s·∫£n ph·∫©m ƒë·ªôc ƒë√°o, c√≥ ti·ªÅm nƒÉng viral, c√≥ th·ªÉ t√¨m ngu·ªìn t·ª´ 1688 trung qu·ªëc, Vietnam, ok cho s·∫£n ph·∫©m c√° nh√¢n h√≥a
6.  **T∆∞ duy v·ªÅ Review Th·∫•p: C∆° h·ªôi trong R·ªßi ro.** kh√¥ng t·ª± ƒë·ªông 'B·ªè qua' m·ªôt s·∫£n ph·∫©m ch·ªâ v√¨ n√≥ c√≥ review th·∫•p. Thay v√†o ƒë√≥, h√£y xem ƒë√¢y l√† m·ªôt **C∆† H·ªòI L·ªöN**. Review th·∫•p nh∆∞ng doanh s·ªë v·∫´n cao cho th·∫•y nhu c·∫ßu th·ªã tr∆∞·ªùng r·∫•t m·∫°nh v√† c√°c ƒë·ªëi th·ªß hi·ªán t·∫°i ƒëang l√†m kh√¥ng t·ªët.
7.  **X·ª≠ l√Ω D·ªØ li·ªáu Thi·∫øu (ƒê·∫∑c bi·ªát l√† C√¢n n·∫∑ng - Weight):** KH√îNG ƒë∆∞·ª£c t·ª± √Ω ∆∞·ªõc t√≠nh ho·∫∑c b·ªãa ra s·ªë li·ªáu n·∫øu c·ªôt ƒë√≥ trong file CSV b·ªã tr·ªëng. Trong tr∆∞·ªùng h·ª£p n√†y, h√£y t·∫≠p trung ph√¢n t√≠ch c√°c y·∫øu t·ªë kh√°c nh∆∞ ti·ªÅm nƒÉng viral, ƒë·ªô ƒë·ªôc ƒë√°o, nhu c·∫ßu th·ªã tr∆∞·ªùng (d·ª±a tr√™n doanh s·ªë/review).


NHI·ªÜM V·ª§:
Ph√¢n t√≠ch L√î s·∫£n ph·∫©m d∆∞·ªõi ƒë√¢y v·ªõi t∆∞ duy c·ªüi m·ªü. M·ª•c ti√™u ch√≠nh c·ªßa m√†y l√† x√°c ƒë·ªãnh c√°c s·∫£n ph·∫©m "winner" ti·ªÅm nƒÉng ti·∫øp theo, b·∫•t k·ªÉ ch√∫ng thu·ªôc danh m·ª•c n√†o. Cung c·∫•p m·ªôt "overall_summary" chi·∫øn l∆∞·ª£c v√† sau ƒë√≥ ƒë√°nh gi√° T·ª™NG s·∫£n ph·∫©m.

D·ªØ li·ªáu l√¥ s·∫£n ph·∫©m (ƒë·ªãnh d·∫°ng CSV):
{product_batch_csv}

Ph√¢n t√≠ch v√† tr·∫£ l·ªùi trong m·ªôt ƒë·ªãnh d·∫°ng JSON h·ª£p l·ªá duy nh·∫•t. KH√îNG th√™m b·∫•t k·ª≥ vƒÉn b·∫£n n√†o b√™n ngo√†i ƒë·ªëi t∆∞·ª£ng JSON ch√≠nh.

C·∫•u tr√∫c ph·∫£n h·ªìi JSON ph·∫£i nh∆∞ sau:
{{
  "overall_summary": "string (T√≥m t·∫Øt chi·∫øn l∆∞·ª£c c·ªßa m√†y v·ªÅ l√¥ h√†ng, nh·∫•n m·∫°nh v√†o c√°c C∆† H·ªòI M·ªöI t·ªët nh·∫•t. Cung c·∫•p ph·∫ßn n√†y b·∫±ng ti·∫øng Vi·ªát.)",
  "product_analyses": [
    {{
      "asin": "string (ASIN c·ªßa s·∫£n ph·∫©m)",
      "classification": "string (Gi·ªØ nguy√™n c√°c thu·∫≠t ng·ªØ ti·∫øng Anh n√†y: Winner, High Potential, Potential, Skip)",
      "reason": "string (L√Ω do to√†n di·ªán c·ªßa m√†y, gi·∫£i th√≠ch t·∫°i sao ƒë√¢y c√≥ th·ªÉ l√† m·ªôt s·∫£n ph·∫©m win, ngay c·∫£ khi n√≥ ·ªü trong m·ªôt ng√°ch m·ªõi. Cung c·∫•p ph·∫ßn n√†y b·∫±ng ti·∫øng Vi·ªát.)",
      "viral_potential": "string (Gi·ªØ nguy√™n c√°c thu·∫≠t ng·ªØ ti·∫øng Anh n√†y: Low, Medium, High)",
      "uniqueness_score": "number (1-10, 1 l√† h√†ng ph·ªï th√¥ng, 10 l√† c·ª±c k·ª≥ ƒë·ªôc ƒë√°o)",
      "market_trend_score": "number (1-10, s·∫£n ph·∫©m n√†y ph√π h·ª£p v·ªõi c√°c xu h∆∞·ªõng hi·ªán t·∫°i ho·∫∑c m·ªõi n·ªïi ·ªü th·ªã tr∆∞·ªùng M·ªπ nh∆∞ th·∫ø n√†o)",
      "logistics_fit": "string (Gi·ªØ nguy√™n c√°c thu·∫≠t ng·ªØ ti·∫øng Anh n√†y: Good, Acceptable, Poor)",
      "estimated_shipping_cost": "number (∆Ø·ªõc t√≠nh c·ªßa m√†y b·∫±ng USD)",
      "profit_potential": "string (Gi·ªØ nguy√™n c√°c thu·∫≠t ng·ªØ ti·∫øng Anh n√†y: Low, Medium, High, Very High)",
      "risks": ["string (Li·ªát k√™ c√°c r·ªßi ro ch√≠nh b·∫±ng ti·∫øng Vi·ªát.)"]
    }}
  ]
}}
"""

# --- CORE FUNCTIONS ---
def analyze_product_batch_api(product_batch_list, model, prompt_template):
    logging.info(f"Bat dau ham analyze_product_batch_api voi {len(product_batch_list)} san pham.") # <<< DEBUG LOG >>>
    if not product_batch_list: return None
    output = io.StringIO()
    fieldnames = product_batch_list[0].keys()
    writer = csv.DictWriter(output, fieldnames=fieldnames)
    writer.writeheader()
    writer.writerows(product_batch_list)
    csv_string = output.getvalue()
    prompt = prompt_template.format(product_batch_csv=csv_string)
    
    # <<< DEBUG LOG >>> In ra prompt ƒë·ªÉ ki·ªÉm tra (ch·ªâ v√†i trƒÉm k√Ω t·ª± ƒë·∫ßu)
    logging.info(f"Prompt da tao (200 ky tu dau): {prompt[:200]}...")
    
    try:
        response = model.generate_content(prompt)
        # <<< DEBUG LOG >>> In ra to√†n b·ªô text tr·∫£ v·ªÅ t·ª´ API TR∆Ø·ªöC khi x·ª≠ l√Ω
        logging.info(f"API Response Text (RAW): {response.text}")
        
        if response.parts:
            cleaned_text = response.text.strip().replace('```json', '').replace('```', '')
            logging.info(f"Cleaned Text (truoc khi load json): {cleaned_text}") # <<< DEBUG LOG >>>
            return json.loads(cleaned_text)
        
        logging.warning("API response khong co 'parts'.") # <<< DEBUG LOG >>>
        return None
    except Exception as e:
        # <<< DEBUG LOG >>> Ghi log l·ªói chi ti·∫øt ra console
        logging.error(f"LOI TRONG HAM analyze_product_batch_api: {e}", exc_info=True)
        st.error(f"L·ªói API ho·∫∑c JSON: {e}")
        return None

# --- STREAMLIT APP UI ---
def main():
    st.title("üöÄ C√¥ng C·ª• Ph√¢n T√≠ch S·∫£n Ph·∫©m")
    st.info("T·∫£i l√™n file CSV t·ª´ Helium 10 ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

    with st.expander("üìñ Xem h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng (Workflow)", expanded=False):
        st.markdown("""
        ...
        """)
    
    # -- Sidebar c√†i ƒë·∫∑t --
    st.sidebar.header("C√†i ƒë·∫∑t Ph√¢n t√≠ch")
    model_name = st.sidebar.selectbox(
        "Ch·ªçn Model Gemini", 
        ('gemini-2.5-pro', 'gemini-1.5-flash'),
        index=0, 
        help="1.5 Pro cho ch·∫•t l∆∞·ª£ng ph√¢n t√≠ch cao nh·∫•t."
    )
    batch_size = st.sidebar.slider("S·ªë s·∫£n ph·∫©m m·ªói l√¥", 5, 20, 10)

    st.sidebar.header("C√†i ƒë·∫∑t Output")
    output_option = st.sidebar.radio("Ch·ªçn ƒë·ªãnh d·∫°ng output:",
                                     ('T·∫£i File CSV', 'Ghi v√†o Google Sheet'),
                                     index=1)
                                     
    write_interval = 1
    if output_option == 'Ghi v√†o Google Sheet':
        gsheet_id = st.sidebar.text_input("ID c·ªßa Google Sheet 'KHO K·∫æT QU·∫¢'", 
                                          "1v0Ms4Mg1L5liXl-5pGRhWoIXSipS0E5z7zGT4G8-JAM")
        write_interval = st.sidebar.slider("Ghi v√†o Google Sheet sau m·ªói (l√¥)", 1, 10, 5,
                                           help="V√≠ d·ª•: ch·ªçn 5 nghƒ©a l√† c·ª© x·ª≠ l√Ω xong 5 l√¥ th√¨ s·∫Ω ghi k·∫øt qu·∫£ v√†o Sheet m·ªôt l·∫ßn.")

    with st.expander("üìù Ch·ªânh s·ª≠a Prompt (N√¢ng cao)"):
        prompt_text = st.text_area("Prompt Template:", value=DEFAULT_PROMPT, height=400)

    uploaded_file = st.file_uploader("T·∫£i file CSV t·ª´ Helium 10 l√™n ƒë√¢y:", type=["csv"])

    if uploaded_file is not None:
        try:
            logging.info(f"Bat dau doc file: {uploaded_file.name}") # <<< DEBUG LOG >>>
            df = pd.read_csv(uploaded_file, low_memory=False)
            logging.info(f"Doc file thanh cong. Shape ban dau: {df.shape}") # <<< DEBUG LOG >>>

            cols_to_drop = [
                'BSR', 
                'UPC', 
                'GTIN', 
                'EAN', 
                'ISBN',
                'Price Trend (90 days) (%)',
                'ASIN Sales',
                'Sales Trend (90 days) (%)',
                'Last Year Sales',
                'Sales Year Over Year (%)',
                'Length',
                'Width',
                'Height',
                'Storage Fee (Jan - Sep)',
                'Storage Fee (Oct - Dec)',
                'Best Sales Period',
                'Sales to Reviews'
            ]
            for col in cols_to_drop:
                if col in df.columns:
                    df = df.drop(columns=[col])
            logging.info(f"Da xoa cac cot thua. Shape sau khi xoa: {df.shape}") # <<< DEBUG LOG >>>

            if 'Weight' in df.columns:
                df['Weight'] = df['Weight'].replace('-', np.nan)
                df['Weight'] = pd.to_numeric(df['Weight'], errors='coerce')
            
            numeric_cols = ['Price', 'Parent Level Sales', 'Revenue', 'Sales', 'BSR']
            for col in numeric_cols:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
            
            df = df.fillna('')
            logging.info("Da don dep va dien vao cac o trong.") # <<< DEBUG LOG >>>

            st.success(f"ƒê√£ t·∫£i l√™n th√†nh c√¥ng file '{uploaded_file.name}' v·ªõi {len(df)} s·∫£n ph·∫©m.")
            st.dataframe(df.head())
            
            num_products = len(df)
            time_per_batch_seconds = 45 
            num_batches = (num_products + batch_size - 1) // batch_size
            total_seconds = num_batches * time_per_batch_seconds
            minutes, seconds = divmod(int(total_seconds), 60)
            
            st.markdown(f"---")
            st.markdown(f"‚è≥ **Th·ªùi gian x·ª≠ l√Ω d·ª± ki·∫øn:** Kho·∫£ng **{minutes} ph√∫t {seconds} gi√¢y** cho `{num_products}` s·∫£n ph·∫©m (`{num_batches}` l√¥).")
            st.caption("Th·ªùi gian th·ª±c t·∫ø c√≥ th·ªÉ thay ƒë·ªïi t√πy v√†o t·∫£i c·ªßa API.")

            if st.button("üöÄ B·∫Øt ƒë·∫ßu Ph√¢n t√≠ch", type="primary", use_container_width=True):
                logging.info("======= NHAN NUT BAT DAU PHAN TICH =======") # <<< DEBUG LOG >>>
                if output_option == 'Ghi v√†o Google Sheet' and (not gsheet_id):
                    st.error("L·ªói: Vui l√≤ng nh·∫≠p ID c·ªßa Google Sheet 'KHO K·∫æT QU·∫¢' v√†o thanh c√†i ƒë·∫∑t b√™n tr√°i.")
                    st.stop()
                
                st.session_state.start_time = time.time()
                progress_bar = st.progress(0, text="ƒêang chu·∫©n b·ªã...")
                timer_placeholder = st.empty()
                timer_placeholder.markdown("‚è≥ **Th·ªùi gian ƒë√£ ch·∫°y:** 00 ph√∫t 00 gi√¢y")
                
                model = genai.GenerativeModel(model_name)
                all_products = df.to_dict('records')
                final_results_for_df = []
                batches = [all_products[i:i + batch_size] for i in range(0, len(all_products), batch_size)]
                logging.info(f"Da chia {num_products} san pham thanh {len(batches)} lo.") # <<< DEBUG LOG >>>
                
                OUTPUT_COLUMN_ORDER = [
                    'ASIN', 'URL', 'Image URL', 'Title', 'Price', 'Parent Level Sales', 'Weight',
                    'classification', 'reason', 'viral_potential', 'uniqueness_score', 
                    'market_trend_score', 'logistics_fit', 'estimated_shipping_cost', 
                    'profit_potential', 'risks'
                ]
                
                INPUT_COLUMNS_FOR_AI = [
                    'ASIN', 'Title', 'Price', 'Parent Level Sales', 'Brand', 'Seller', 
                    'Seller Country/Region', 'Weight', 'Age (Month)', 'Review Count', 
                    'Reviews Rating', 'Category', 'Subcategory', 'Number of Images', 'Variation Count'
                ]
                
                worksheet = None
                next_row_to_write = 2 

                # <<< START: LOGIC M·ªöI - CHU·∫®N B·ªä GOOGLE SHEET TR∆Ø·ªöC V√íNG L·∫∂P >>>
                if output_option == 'Ghi v√†o Google Sheet':
                    with st.spinner("ƒêang chu·∫©n b·ªã Google Sheet..."):
                        try:
                            timestamp = datetime.now().strftime("%y%m%d_%H%M")
                            # R√∫t g·ªçn t√™n file ƒë·ªÉ kh√¥ng qu√° d√†i
                            file_base_name = os.path.splitext(uploaded_file.name)[0][:30] 
                            worksheet_name = f"{timestamp}_{file_base_name}"
                            
                            spreadsheet = gc.open_by_key(gsheet_id)
                            # Sao ch√©p t·ª´ sheet Template n·∫øu c√≥, n·∫øu kh√¥ng th√¨ t·∫°o m·ªõi
                            try:
                                template_sheet = spreadsheet.worksheet("Template")
                                worksheet = spreadsheet.duplicate_sheet(source_sheet_id=template_sheet.id, new_sheet_name=worksheet_name)
                            except gspread.exceptions.WorksheetNotFound:
                                st.info("Kh√¥ng t√¨m th·∫•y sheet 'Template'. S·∫Ω t·∫°o sheet m·ªõi.")
                                worksheet = spreadsheet.add_worksheet(title=worksheet_name, rows=1, cols=len(OUTPUT_COLUMN_ORDER))
                            
                            # Ghi header v√†o sheet m·ªõi
                            # Ghi t·ª´ √¥ A1
                            worksheet.update(range_name='B1', values=[OUTPUT_COLUMN_ORDER]) 
                            st.success(f"ƒê√£ t·∫°o worksheet '{worksheet_name}' v√† ghi header t·ª´ √¥ B1 th√†nh c√¥ng.")
                        except Exception as e:
                            st.error(f"L·ªói khi chu·∫©n b·ªã Google Sheet: {e}")
                            st.stop()
                # <<< END: LOGIC M·ªöI - CHU·∫®N B·ªä GOOGLE SHEET >>>

                batch_results_for_sheet = [] # List ƒë·ªÉ t√≠ch tr·ªØ k·∫øt qu·∫£ cho m·ªói l·∫ßn ghi v√†o sheet
                for i, batch in enumerate(batches):
                    logging.info(f"--- Bat dau xu ly lo {i+1}/{len(batches)} ---") # <<< DEBUG LOG >>>
                    
                    elapsed_seconds = time.time() - st.session_state.start_time
                    mins, secs = divmod(int(elapsed_seconds), 60)
                    timer_text = f"‚è≥ **Th·ªùi gian ƒë√£ ch·∫°y:** {mins:02d} ph√∫t {secs:02d} gi√¢y"
                    timer_placeholder.markdown(timer_text)

                    status_text = f"ƒêang x·ª≠ l√Ω l√¥ {i+1}/{len(batches)}... Vui l√≤ng ch·ªù."
                    progress_bar.progress((i) / len(batches), text=status_text)

                    if not batch: 
                        logging.warning(f"Lo {i+1} rong, bo qua.") # <<< DEBUG LOG >>>
                        continue
                        
                    filtered_batch = []
                    for product in batch:
                        # product ·ªü ƒë√¢y l√† m·ªôt dict ch·ª©a T·∫§T C·∫¢ th√¥ng tin
                        # filtered_product l√† m·ªôt dict m·ªõi ch·ªâ ch·ª©a c√°c key trong INPUT_COLUMNS_FOR_AI
                        filtered_product = {key: product.get(key, '') for key in INPUT_COLUMNS_FOR_AI}
                        filtered_batch.append(filtered_product)
                    
                    # Ch·ªâ g·ª≠i c√°i batch ƒë√£ ƒë∆∞·ª£c l·ªçc ƒëi ph√¢n t√≠ch
                    analysis_result = analyze_product_batch_api(filtered_batch, model, prompt_text)
                    time.sleep(2)
                    
                    # <<< DEBUG LOG >>> In ra ket qua phan tich cua lo
                    logging.info(f"Ket qua phan tich cho lo {i+1}: {analysis_result}")
                    
                    processed_batch = []
                    if analysis_result and 'product_analyses' in analysis_result:
                        logging.info(f"Lo {i+1} phan tich thanh cong, bat dau ghep du lieu.") # <<< DEBUG LOG >>>
                        batch_map = {str(row.get('ASIN')): row for row in batch if row.get('ASIN')}
                        for analysis in analysis_result['product_analyses']:
                            asin = str(analysis.get('asin'))
                            if asin in batch_map:
                                original_row = batch_map[asin]
                                original_row.update(analysis)
                                if isinstance(original_row.get('risks'), list):
                                    original_row['risks'] = ', '.join(map(str, original_row['risks']))
                                processed_batch.append(original_row)
                            else:
                                # <<< DEBUG LOG >>> Canh bao neu ASIN tu AI khong co trong batch goc
                                logging.warning(f"ASIN '{asin}' tu AI response khong tim thay trong batch goc.")
                    else:
                        logging.error(f"LOI: Lo {i+1} xu ly that bai hoac khong co ket qua. Gan co 'API_ERROR'.") # <<< DEBUG LOG >>>
                        st.warning(f"L√¥ {i+1} x·ª≠ l√Ω th·∫•t b·∫°i ho·∫∑c kh√¥ng c√≥ k·∫øt qu·∫£. G·∫Øn c·ªù 'API_ERROR'.")
                        for row in batch:
                            row['classification'] = 'API_ERROR'
                            processed_batch.append(row)
                    
                    final_results_for_df.extend(processed_batch)
                    
                    if output_option == 'Ghi v√†o Google Sheet' and worksheet:
                        batch_results_for_sheet.extend(processed_batch)
                        
                        # ƒêi·ªÅu ki·ªán ƒë·ªÉ ghi:
                        # 1. ƒê√£ x·ª≠ l√Ω ƒë·ªß s·ªë l√¥ trong `write_interval`
                        # 2. Ho·∫∑c ƒë√¢y l√† l√¥ cu·ªëi c√πng
                        is_last_batch = (i + 1) == len(batches)
                        if (len(batch_results_for_sheet) > 0) and (((i + 1) % write_interval == 0) or is_last_batch):
                            with st.spinner(f"ƒêang ghi {len(batch_results_for_sheet)} k·∫øt qu·∫£ v√†o Google Sheet..."):
                                try:
                                    # Chuy·ªÉn list of dicts th√†nh list of lists theo ƒë√∫ng th·ª© t·ª± c·ªôt
                                    # S·ª≠a l·∫°i cho ƒë√∫ng
                                    rows_to_append = []
                                    for row_dict in batch_results_for_sheet:
                                        row_list = [row_dict.get(col, '') for col in OUTPUT_COLUMN_ORDER]
                                        rows_to_append.append(row_list)                                 
                                    range_to_update = f'B{next_row_to_write}'
                                    worksheet.update(range_name=range_to_update, values=rows_to_append)

                                    next_row_to_write += len(rows_to_append)

                                    st.toast(f"‚úÖ ƒê√£ ghi th√†nh c√¥ng {len(batch_results_for_sheet)} s·∫£n ph·∫©m v√†o Sheet!")
                                    batch_results_for_sheet.clear()
                                except Exception as e:
                                    st.error(f"L·ªói khi ghi batch v√†o Google Sheet: {e}")
                                    # Gi·ªØ l·∫°i d·ªØ li·ªáu ƒë·ªÉ th·ª≠ ghi v√†o l·∫ßn sau
                    # <<< END: LOGIC GHI BATCH >>>

                progress_bar.progress(1.0, text="Ph√¢n t√≠ch AI ho√†n t·∫•t!")
                logging.info("======= HOAN TAT TOAN BO CAC LO =======") # <<< DEBUG LOG >>>

                if not final_results_for_df:
                    st.warning("Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë∆∞·ª£c t·∫°o ra. Vui l√≤ng ki·ªÉm tra l·∫°i file input ho·∫∑c c√†i ƒë·∫∑t.")
                    st.stop()
                
                result_df = pd.DataFrame(final_results_for_df)
                final_ordered_columns = [col for col in OUTPUT_COLUMN_ORDER if col in result_df.columns]
                other_columns = [col for col in result_df.columns if col not in final_ordered_columns]
                result_df = result_df[final_ordered_columns + other_columns]
                
                st.subheader("Xem Tr∆∞·ªõc To√†n B·ªô K·∫øt Qu·∫£ Ph√¢n T√≠ch")
                st.dataframe(result_df)

                if output_option == 'T·∫£i File CSV':
                    with st.spinner("ƒêang chu·∫©n b·ªã file CSV..."):
                        csv_output = result_df.to_csv(index=False).encode('utf-8-sig')
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        output_filename = f"{timestamp}_analyzed_{uploaded_file.name}"
                        st.download_button(
                            label="üì• T·∫£i v·ªÅ file k·∫øt qu·∫£ CSV",
                            data=csv_output,
                            file_name=output_filename,
                            mime='text/csv',
                            use_container_width=True
                        )
                
                elif output_option == 'Ghi v√†o Google Sheet' and worksheet:
                    sheet_url = f"https://docs.google.com/spreadsheets/d/{gsheet_id}/edit#gid={worksheet.id}"
                    st.balloons()
                    st.success("üéâ Ghi d·ªØ li·ªáu v√†o tab m·ªõi th√†nh c√¥ng!")
                    st.link_button("M·ªü Google Sheet", url=sheet_url, use_container_width=True)

        except Exception as e:
            # <<< DEBUG LOG >>> Ghi log loi tong the cua qua trinh
            logging.error(f"LOI TONG THE trong luc xu ly file: {e}", exc_info=True)
            st.error(f"ƒê√£ c√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh x·ª≠ l√Ω file: {e}")
            import traceback
            st.code(traceback.format_exc())

if __name__ == "__main__":
    main()